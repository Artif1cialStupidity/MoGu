# generate randomly initialized surrogate model
python defenses/victim/train.py ImageNet1k resnet50 --out_path models/victim/ImageNet1k-resnet50-train-nodefense-scratch-advproxy --device_id 0 --epochs 1 --train_subset 10 --lr 0.0

################################ ImageNet1k ################################
### If you have multiple GPUs on the machine, use this to select the specific GPU
dev_id=3
### p_v = victim model dataset
p_v=ImageNet1k
### f_v = architecture of victim model
f_v=resnet50
### queryset = p_a = image pool of the attacker 
queryset=ImageNet1k
### Path to victim model's directory (the one downloded earlier)
vic_dir=models/victim/${p_v}-${f_v}-train-nodefense
### No. of images queried by the attacker. With 60k, attacker obtains 99.05% test accuracy on MNIST at eps=0.0.
budget=50000 
### Batch size of queries to process for the attacker
batch_size=8
### Training parameters
lr=0.001
lr_step=3
lr_gamma=0.5
epochs=10
training_batch_size=32

### attack policy
## pretrained model
# use imagenet pretrained
pretrained=imagenet

# use optimal pretrained
pretrained=${vic_dir}

## random
policy=random
defense_aware=0
policy_suffix="_da${defense_aware}"


## jbda/jbself/jbtop
policy=jbtop3
seedsize=500
epsilon=0.1
T=8
defense_aware=1
policy_suffix="_da${defense_aware}_ss${seedsize}_eps${epsilon}"

## semisupervised setting
semi_train_weight=0.0
semi_dataset=${queryset}

### Defense strategy
## Quantization
quantize=0
quantize_epsilon=0.0
optim=approx
frozen=1
ydist=l1
quantize_args="epsilon:${quantize_epsilon};ydist:${ydist};optim:${optim};trainingset_name:${p_v};frozen:${frozen}"

## None
strat=none
# Output path to attacker's model
out_dir=models/final_bb_dist/${p_v}-${f_v}/${policy}${policy_suffix}-${queryset}-B${budget}/none/quantize${quantize_epsilon}
# Parameters to defense strategy, provided as a key:value pair string. 
defense_args="out_path:${out_dir}"

## MAD
strat=mad
# Metric for perturbation ball dist(y, y'). Supported = L1, L2, KL
ydist=l1
# Perturbation norm
eps=1.0

# version 1: original MAD
batch_size=1
# Perturbation mode: extreme|argmax
oracle=argmax
# Initialization to the defender's surrogate model. 'scratch' refers to random initialization.
proxystate=scratch
# Path to surrogate model
proxydir=models/victim/${p_v}-${f_v}-train-nodefense-${proxystate}-advproxy
# Output path to attacker's model
out_dir=models/final_bb_dist/${p_v}-${f_v}/${policy}${policy_suffix}-${queryset}-B${budget}/mad_${oracle}_${ydist}/eps[${eps},${quantize_epsilon}]-proxy_${proxystate}
# Parameters to defense strategy, provided as a key:value pair string. 
defense_args="epsilon:${eps};batch_constraint:0;objmax:1;oracle:${oracle};ydist:${ydist};model_adv_proxy:${proxydir};out_path:${out_dir}"

# version 2: LP-based MAD
# Perturbation mode: lp_extreme|lp_argmax
oracle=lp_argmax
# Using Batch Constraint
batch_constraint=0
# Initialization to the defender's surrogate model. 'scratch' refers to random initialization.
proxystate=scratch
# Path to surrogate model
proxydir=models/victim/${p_v}-${f_v}-train-nodefense-${proxystate}-advproxy
# Output path to attacker's model
out_dir=models/final_bb_dist/${p_v}-${f_v}/${policy}${policy_suffix}-${queryset}-B${budget}/mad_${oracle}_${ydist}/batch${batch_size}-eps[${eps},${quantize_epsilon}]_bc${batch_constraint}-proxy_${proxystate}
# Parameters to defense strategy, provided as a key:value pair string. 
defense_args="epsilon:${eps};batch_constraint:${batch_constraint};objmax:1;oracle:${oracle};ydist:${ydist};model_adv_proxy:${proxydir};out_path:${out_dir}"

# Version 3: MUAD (Use I as jacobian in MAD)
# Output path to attacker's model
out_dir=models/final_bb_dist/${p_v}-${f_v}/${policy}${policy_suffix}-${queryset}-B${budget}/muad_${oracle}_${ydist}/batch${batch_size}-eps[${eps},${quantize_epsilon}]_bc${batch_constraint}
# Parameters to defense strategy, provided as a key:value pair string. 
defense_args="epsilon:${eps};batch_constraint:${batch_constraint};objmax:1;oracle:${oracle};ydist:${ydist};disable_jacobian:1;out_path:${out_dir}"

## MLD
strat=mld
# Using Batch Constraint
batch_constraint=0
# Metric for perturbation ball dist(y, y'). Supported = L1, L2, KL
ydist=l1
# Perturbation norm
eps=0.5
# Output path to attacker's model
out_dir=models/final_bb_dist/${p_v}-${f_v}/${policy}${policy_suffix}-${queryset}-B${budget}/mld_${ydist}/batch${batch_size}-eps[${eps},${quantize_epsilon}]_bc${batch_constraint}
# Parameters to defense strategy, provided as a key:value pair string. 
defense_args="epsilon:${eps};batch_constraint:${batch_constraint};ydist:${ydist};out_path:${out_dir}"

## random noise
strat=rand_noise
# Metric for perturbation ball dist(y, y'). Supported = L1, L2, KL
zdist=l1
# Perturbation norm
eps=10.0
# Output path to attacker's model
out_dir=models/final_bb_dist/${p_v}-${f_v}/${policy}${policy_suffix}-${queryset}-B${budget}/rand_${zdist}/eps[${eps},${quantize_epsilon}]
# Parameters to defense strategy, provided as a key:value pair string. 
defense_args="epsilon_z:${eps};dist_z:${zdist};out_path:${out_dir}"

## topk
# Output path to attacker's model
strat=topk
topk=1
rounding=0
out_dir=models/final_bb_dist/${p_v}-${f_v}/${policy}${policy_suffix}-${queryset}-B${budget}/top${topk}-rounding${rounding}
defense_args="topk:${topk};rounding:${rounding};out_path:${out_dir}"

## reverse sigmoid
# Output path to attacker's model
strat=reverse_sigmoid
beta=0.005
gamma=0.2
out_dir=models/final_bb_dist/${p_v}-${f_v}/${policy}${policy_suffix}-${queryset}-B${budget}/revsig/beta${beta}-gamma${gamma}-quantize${quantize_epsilon}
defense_args="beta:${beta};gamma:${gamma};out_path:${out_dir}"

### Execution commands
# (defense) train an original (blackbox) model
python defenses/victim/train.py ${p_v} ${f_v} -o ${vic_dir} --epochs 1 -b 64 --train_subset 10 --lr 0.0 -d ${dev_id} --pretrained ${pretrained}

# (defense) evaluate original model with defense on testset
python defenses/victim/eval.py ${vic_dir} ${strat} ${defense_args} --out_dir ${out_dir} --batch_size ${batch_size} -d ${dev_id}

# (adversary) generate transfer dataset (only when policy=random)
python defenses/adversary/transfer.py ${policy} ${vic_dir} ${strat} ${defense_args} --out_dir ${out_dir} --batch_size ${batch_size} -d ${dev_id} --queryset ${queryset} --budget ${budget} --defense_aware ${defense_aware} --quantize ${quantize} --quantize_args ${quantize_args}

# (adversary) train kickoffnet and evaluate
python defenses/adversary/train.py ${out_dir} ${f_v} ${p_v} --budgets 50000 -e ${epochs} -b ${training_batch_size} --lr ${lr} --lr_step ${lr_step} --lr_gamma ${lr_gamma} -d ${dev_id} -w 4 --vic_dir ${vic_dir} --semitrainweight ${semi_train_weight} --semidataset ${semi_dataset}

# (adversary) use jbda/jbda-topk as attack policy
python defenses/adversary/jacobian.py ${policy} ${vic_dir} ${strat} ${defense_args} --model_adv ${f_v} --pretrained imagenet --out_dir ${out_dir} --testset ${p_v} --batch_size 128 -d ${dev_id} --queryset ${queryset} --budget ${budget} --seedsize ${seedsize} --epsilon ${epsilon} --T ${T} --train_epochs 20